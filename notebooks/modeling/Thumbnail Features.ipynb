{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray, gray2rgb, rgb2hsv\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facial_detection_haar(filename):\n",
    "    img = io.imread(filename)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=3,\n",
    "        minSize=(30, 30),\n",
    "        flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 10)\n",
    "    return img, len(faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facial_recognition(filename, cnn=True):\n",
    "    image = face_recognition.load_image_file(filename)\n",
    "    if cnn:\n",
    "        face_locations = face_recognition.face_locations(image, model='cnn')\n",
    "    else:\n",
    "        face_locations = face_recognition.face_locations(image)\n",
    "    return image, len(face_locations), face_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facial_analysis(face_locations,image,config='age, gender, race, emotion'):\n",
    "    if len(face_locations)>0:\n",
    "        config = config.split(',')\n",
    "        config = [c.strip() for c in config]\n",
    "        results = []\n",
    "        im = Image.fromarray(image)\n",
    "        for f in face_locations:\n",
    "            face = im.crop((f[3],f[0],f[1],f[2]))\n",
    "            face = np.asarray(face)\n",
    "            demography = DeepFace.analyze(face,config)\n",
    "            results.append(demography)\n",
    "        return results, face_locations\n",
    "    else:\n",
    "        return [],face_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facial_percentage(face_locations,image):\n",
    "    result = []\n",
    "    img_pixelcount = image_obj.shape[0]*image_obj.shape[1]\n",
    "    for f in face_locations:\n",
    "        face_pixels = (f[2]-f[0])*(f[1]-f[3])\n",
    "        result.append(face_pixels/img_pixelcount)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_database(IMG_DIR):\n",
    "    cols = ['videoId','numFaces','emotions','face_locations','face_percent']\n",
    "    feature_df = pd.DataFrame(columns=cols)\n",
    "    for filename in os.listdir(IMG_DIR):\n",
    "        genders = []\n",
    "        image_obj,num_faces,face_coords = facial_recognition(IMG_DIR+'/'+filename)\n",
    "        #face locations coordinates are (top, right, bottom, left)\n",
    "        analysis,face_locations = facial_analysis(face_coords,image_obj)\n",
    "        if len(analysis)>0:\n",
    "            emotions = [f['dominant_emotion'] for f in analysis]\n",
    "            age = [f['age'] for f in analysis]\n",
    "            gender = [f['gender'] for f in analysis]\n",
    "            race = [f['dominant_race'] for f in analysis]\n",
    "        else:\n",
    "            emotions=age=gender=race=np.nan\n",
    "            \n",
    "        face_percent = facial_percentage(image_obj,face_coords)\n",
    "        feature_df = feature_df.append({'videoId':filename[:-4],'numFaces':num_faces,'emotions':emotions,'age':age,\n",
    "                                        'gender':gender,'race':race,'face_locations':face_locations,\n",
    "                                        'face_percent':face_percent}, ignore_index=True)\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
