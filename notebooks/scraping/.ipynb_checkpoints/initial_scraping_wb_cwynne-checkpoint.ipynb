{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Scrape Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We ideally want to scrape a dataset consisting of\n",
    "- video thumbnails\n",
    "- titles\n",
    "- views\n",
    "- parent channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YouTube API\n",
    "https://developers.google.com/youtube/v3/getting-started#quota\n",
    "\n",
    "Get credentials by going to https://console.developers.google.com/apis/dashboard?project=red-means-go\n",
    "- Look for \"YouTube Data API v3\" in the library tab and make sure it's enabled.\n",
    "- Select Credentials and get an api key\n",
    "\n",
    "Daily limit of 10,000 \"units\" worth of requests.\n",
    "- Different operations have different cost values, need to be careful what data we request.\n",
    "\n",
    "We can more efficiently get data by using the offered compressed gzip request format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install --upgrade google-auth-oauthlib google-auth-httplib2\n",
    "!pip install --upgrade google-api-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desired scraping code\n",
    "- config files to identify what categories of videos to scrape\n",
    "- what level of popularity to lower bound our videos to\n",
    "    - what measurement works for this? subscription to yearly average view count in relation to videos uploaded count?\n",
    "- possible inversion config option to instead opt for getting the least popular videos(?)\n",
    "- output to data/out/\n",
    "    - /thumbs -- a folder full of thumbnails with identifying labels (possibly gzip compressed?)\n",
    "    - videos.csv -- a .csv containing metadata on the videos that correspond to the thumbnails in the above folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible search parameters\n",
    "- Safesearch\n",
    "    - none\n",
    "    - moderate\n",
    "    - strict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup and Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopes = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "with open('../../api_key.json') as json_file:\n",
    "    cred = json.load(json_file)\n",
    "api_key = cred['api_key']\n",
    "# Disable OAuthlib's HTTPS verification when running locally.\n",
    "# *DO NOT* leave this option enabled in production.\n",
    "os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_playlist_videos(playlist_id, num_results, page_token=None):\n",
    "    if page_token:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"snippet\",\n",
    "            maxResults=num_results,\n",
    "            playlistId=playlist_id,\n",
    "            pageToken=page_token\n",
    "        )\n",
    "    else:\n",
    "        request = youtube.playlistItems().list(\n",
    "                part=\"snippet\",\n",
    "                maxResults=num_results,\n",
    "                playlistId=playlist_id,\n",
    "            )\n",
    "    response = request.execute()\n",
    "    return response\n",
    "\n",
    "def request_topic_id(q_term): \n",
    "    request = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            q=q_term,\n",
    "            maxResults=\"1\",\n",
    "            type=\"channel\",\n",
    "            order=\"relevance\"\n",
    "            )\n",
    "    response = request.execute()\n",
    "    game_topic_channel = response['items'][0]['snippet']['channelId']\n",
    "    return game_topic_channel\n",
    "\n",
    "def request_recent_playlist_id(game_topic_channel):\n",
    "    request = youtube.channelSections().list(\n",
    "        part=\"snippet,contentDetails\",\n",
    "        channelId=game_topic_channel,\n",
    "        )\n",
    "    response = request.execute()\n",
    "    recent_playlist = None\n",
    "    for section in response['items']:\n",
    "        try: \n",
    "            if section['snippet']['localized']['title'] == \"Recent Videos\":\n",
    "                recent_playlist = section['contentDetails']['playlists'][0]\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "    return recent_playlist\n",
    "\n",
    "def generate_dataset(q_term, num_channels, videos_per_channel):\n",
    "    topic_id = request_topic_id(q_term + \" topic\")\n",
    "    topic_recent_playlist_id = request_recent_playlist_id(topic_id)\n",
    "    recent_playlist_details = request_playlist_videos(topic_recent_playlist_id,5)\n",
    "    video_ids = get_video_ids(topic_recent_playlist_id, recent_playlist_details, num_channels)\n",
    "    parent_ids = get_parent_channels(video_ids)\n",
    "    channel_videos_dic = populate_channel_game_videos(q_term, parent_ids)\n",
    "    res = generate_result_dics(video_ids, parent_ids, channel_videos_dic)\n",
    "    return res\n",
    "\n",
    "def get_video_ids(playlist_id, playlist_details, num_vids):\n",
    "    recent_video_ids = []\n",
    "    for vid_data in playlist_details['items']:\n",
    "        recent_video_ids.append(vid_data['snippet']['resourceId']['videoId'])\n",
    "        if len(recent_video_ids) == num_vids:\n",
    "            break\n",
    "    next_token = playlist_details['nextPageToken']\n",
    "    cur_page = request_playlist_videos(playlist_id, 5, next_token)\n",
    "    while len(recent_video_ids) < num_vids:\n",
    "        cur_page = request_playlist_videos(playlist_id, 5, next_token)\n",
    "        for vid_data in cur_page['items']:\n",
    "            recent_video_ids.append(vid_data['snippet']['resourceId']['videoId'])\n",
    "            if len(recent_video_ids) == num_vids:\n",
    "                break\n",
    "        next_token = cur_page['nextPageToken']\n",
    "    return recent_video_ids\n",
    "\n",
    "def get_parent_channels(video_ids):\n",
    "    parent_channel_ids = []\n",
    "    for vid_id in video_ids:\n",
    "        vid_content = request_sparse_video_details(vid_id)\n",
    "        parent_channel = vid_content['items'][0]['snippet']['channelId']\n",
    "        parent_channel_ids.append(parent_channel)\n",
    "    return parent_channel_ids\n",
    "\n",
    "def request_sparse_video_details(video_id):\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey=api_key)\n",
    "    # note that this uses youtube.videos instead of youtube.search\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response     \n",
    "\n",
    "def get_channel_game_videos(game, parent):\n",
    "    request = youtube.channels().list(\n",
    "    part=\"snippet,contentDetails\",\n",
    "    id=parent,\n",
    "    )\n",
    "    response = request.execute()\n",
    "    uploads_id = response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    uploads_details = request_playlist_videos(uploads_id, 50)\n",
    "    game_vids = []\n",
    "    for vid_data in uploads_details['items']:\n",
    "        vid_title = vid_data['snippet']['title'].lower()\n",
    "        vid_desc = vid_data['snippet']['description'].lower()\n",
    "        if game in vid_title or game in vid_desc:\n",
    "            game_vids.append(vid_data['snippet']['resourceId']['videoId'])\n",
    "    return game_vids\n",
    "\n",
    "def populate_channel_game_videos(game, parents):\n",
    "    channel_videos = {}\n",
    "    for par_chan in parents:\n",
    "        if par_chan not in channel_videos.keys():\n",
    "            channel_videos[par_chan] = get_channel_game_videos(game, par_chan)\n",
    "        else:\n",
    "            continue\n",
    "    return channel_videos\n",
    "\n",
    "def generate_result_dics(videos, parents, channel_videos):\n",
    "    all_results = []\n",
    "    for i in range(len(parents)):\n",
    "        out_dic = {\"video_id\": videos[i],\n",
    "                   \"position\": i,\n",
    "                   \"channel_id\": parents[i],\n",
    "                   \"channel_videos\": channel_videos[parents[i]]}\n",
    "        all_results.append(out_dic)\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortnite = generate_dataset(\"fortnite\", 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "apex = generate_dataset(\"apex legends\", 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Functions / Code that is no longer in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def general_search(q_term, num_results, res_type, channel_id, order_type, page_token=None):\n",
    "#     youtube = googleapiclient.discovery.build(\n",
    "#         api_service_name, api_version, developerKey=api_key)\n",
    "    \n",
    "#     if page_token:\n",
    "#         request = youtube.search().list(\n",
    "#             part=\"snippet\",\n",
    "#             maxResults=num_results,\n",
    "#             type=res_type,\n",
    "#             channelId=channel_id,\n",
    "#             order=order_type,\n",
    "#             pageToken=page_token\n",
    "#         )\n",
    "#     else:\n",
    "#         request = youtube.search().list(\n",
    "#             part=\"snippet\",\n",
    "#             maxResults=num_results,\n",
    "#             type=res_type,\n",
    "#             channelId=channel_id,\n",
    "#             order=order_type\n",
    "#         )\n",
    "#     response = request.execute()\n",
    "#     return response\n",
    "\n",
    "# def request_channels(q_term, num_results,order_type):\n",
    "#     # all arguments must be strings, returns json object with list of channels\n",
    "#     youtube = googleapiclient.discovery.build(\n",
    "#         api_service_name, api_version, developerKey=api_key)\n",
    "    \n",
    "#     # Search parameters\n",
    "#     request = youtube.search().list(\n",
    "#         part=\"snippet\",\n",
    "#         q=q_term,\n",
    "#         maxResults=num_results,\n",
    "#         type=\"channel\",\n",
    "#         order=order_type\n",
    "#     )\n",
    "#     response = request.execute()\n",
    "\n",
    "#     return response\n",
    "\n",
    "# def request_channel_videos(channel_id, num_results, order_type, page_token):\n",
    "#     # all arguments must be strings, returns json object with list of videos from the given channel\n",
    "    \n",
    "#     youtube = googleapiclient.discovery.build(\n",
    "#         api_service_name, api_version, developerKey=api_key)\n",
    "#     if page_token:\n",
    "#         request = youtube.search().list(\n",
    "#             part=\"snippet\",\n",
    "#             maxResults=num_results,\n",
    "#             type=\"video\",\n",
    "#             channelId=channel_id,\n",
    "#             order=order_type,\n",
    "#             pageToken=page_token\n",
    "#         )\n",
    "#     else:\n",
    "#         request = youtube.search().list(\n",
    "#             part=\"snippet\",\n",
    "#             maxResults=num_results,\n",
    "#             type=\"video\",\n",
    "#             channelId=channel_id,\n",
    "#             order=order_type\n",
    "#         )\n",
    "#     response = request.execute()\n",
    "#     return response\n",
    "\n",
    "# def request_video_details(video_id):\n",
    "#     youtube = googleapiclient.discovery.build(\n",
    "#         api_service_name, api_version, developerKey=api_key)\n",
    "#     # note that this uses youtube.videos instead of youtube.search\n",
    "#     request = youtube.videos().list(\n",
    "#         part=\"snippet,contentDetails,statistics\",\n",
    "#         id=video_id\n",
    "#     )\n",
    "#     response = request.execute()\n",
    "#     return response\n",
    "\n",
    "# def get_vid_stats(vid):\n",
    "#     channel_id = vid['snippet']['channelId']\n",
    "#     channel_title = vid['snippet']['channelTitle']\n",
    "#     try:\n",
    "#         thumbnail_link = vid['snippet']['thumbnails']['maxres']['url']\n",
    "#     except:\n",
    "#         thumbnail_link = vid['snippet']['thumbnails']['high']['url']\n",
    "#     title = vid['snippet']['title']\n",
    "#     date = vid['snippet']['publishedAt']\n",
    "#     views = vid['statistics']['viewCount']\n",
    "#     likes = vid['statistics']['likeCount']\n",
    "#     dislikes = vid['statistics']['dislikeCount']\n",
    "#     comments = vid['statistics']['commentCount']\n",
    "#     stats = [channel_id, channel_title, thumbnail_link, title, date, views, likes, dislikes, comments]\n",
    "#     return stats\n",
    "\n",
    "# # full pipeline for scraping and generating dataframe\n",
    "# def main_pipeline():\n",
    "#     metadata = []\n",
    "#     # get initial search results (usually going to be a list of channels)\n",
    "#     out = request_channels(\"gaming\",\"5\",\"relevance\")\n",
    "#     data = out['items']\n",
    "#     # get channel ids from search results\n",
    "#     channel_ids = []\n",
    "#     for channel in data:\n",
    "#         cur_channel_id = channel['snippet']['channelId']\n",
    "#         channel_ids.append(cur_channel_id)\n",
    "#         # get channel videos from the current channel id (we can also choose to handpick channels / videos)\n",
    "#         videos = request_channel_videos(cur_channel_id,\"5\",\"date\",None)\n",
    "#         video_ids = []\n",
    "#         for video in videos['items']:\n",
    "#             cur_id = video['id']['videoId']\n",
    "#             video_ids.append(cur_id)\n",
    "#             # use youtube videos api to get metadata about a single video, by video id\n",
    "#             cur_vid = request_video_details(cur_id)['items'][0]\n",
    "#             row = get_vid_stats(cur_vid)\n",
    "#             metadata.append(row)\n",
    "#         time.sleep(1)\n",
    "\n",
    "#     # create a dataframe from the gathered metadata\n",
    "#     columns = ['channelId','channelTitle','thumbnailLink',\n",
    "#                'videoTitle','Date','Views',\n",
    "#                'Likes','Dislikes','Comments']\n",
    "#     df = pd.DataFrame(metadata,columns=columns)\n",
    "#     return df\n",
    "\n",
    "# def get_channel_id(q_term):\n",
    "#     # rough channel conversion function (may not work all the time)\n",
    "    \n",
    "#     init_search = request_channels(q_term,\"1\",\"relevance\")\n",
    "#     chan_id = init_search['items'][0]['snippet']['channelId']\n",
    "#     return chan_id\n",
    "\n",
    "# def generate_video_data(video_list):\n",
    "#     # generates video data for each video in given list and returns a formatted df of video statistics\n",
    "#     channel_data = []\n",
    "#     for vid in video_list['items']:\n",
    "#         cur_id = vid['id']['videoId']\n",
    "#         cur_vid = request_video_details(cur_id)['items'][0]\n",
    "#         row = get_vid_stats(cur_vid)\n",
    "#         channel_data.append(row)\n",
    "#     columns = ['channelId','channelTitle','thumbnailLink',\n",
    "#                'videoTitle','Date','Views',\n",
    "#                'Likes','Dislikes','Comments']\n",
    "#     df = pd.DataFrame(channel_data,columns=columns)\n",
    "#     df['Views'] = df['Views'].astype(int)\n",
    "#     df['Likes'] = df['Likes'].astype(int)\n",
    "#     df['Dislikes'] = df['Dislikes'].astype(int)\n",
    "#     df['Comments'] = df['Comments'].astype(int)\n",
    "#     return df\n",
    "\n",
    "# def generate_thumbnails(thumbnail_list):\n",
    "#     # just makes a request call to the thumbnail link\n",
    "#     imgs = []\n",
    "#     for link in thumbnail_list:\n",
    "#         response = requests.get(link)\n",
    "#         img = Image.open(BytesIO(response.content))\n",
    "#         imgs.append(img)\n",
    "#     return imgs\n",
    "\n",
    "# def evaluate_videos(df):\n",
    "#     # evaulated based on z score of views and likes currently, very rough approx\n",
    "#     avg_views = df['Views'].mean()\n",
    "#     avg_likes = df['Likes'].mean()\n",
    "#     avg_dislikes = df['Dislikes'].mean()\n",
    "#     avg_comments = df['Comments'].mean()\n",
    "    \n",
    "#     std_views = np.std(df['Views'])\n",
    "#     std_likes = np.std(df['Likes'])\n",
    "#     std_dislikes = np.std(df['Dislikes'])\n",
    "#     std_comments = np.std(df['Comments'])\n",
    "    \n",
    "#     z_views = df['Views'].apply(lambda x: (x - avg_views) / std_views)\n",
    "#     z_likes = df['Likes'].apply(lambda x: (x - avg_likes) / std_likes)\n",
    "#     z_dislikes = df['Dislikes'].apply(lambda x: (x - avg_dislikes) / std_dislikes)\n",
    "#     z_comments = df['Comments'].apply(lambda x: (x - avg_comments) / std_comments)\n",
    "    \n",
    "#     good_videos = []\n",
    "#     bad_videos = []\n",
    "#     for i in range(len(df)):\n",
    "#         if z_views[i] > .3 and z_likes[i] > .3:\n",
    "#             good_videos.append(i)\n",
    "#         if z_views[i] < -.5 and z_likes[i] < -.5:\n",
    "#             bad_videos.append(i)\n",
    "\n",
    "#     return good_videos, bad_videos\n",
    "\n",
    "# def analyze_channel(keyword):\n",
    "#     channel_id = get_channel_id(keyword)\n",
    "#     channel_videos = request_channel_videos(channel_id, \"30\", \"date\", None)\n",
    "#     video_data_df = generate_video_data(channel_videos)\n",
    "#     video_thumbnails = generate_thumbnails(video_data_df['thumbnailLink'].values)\n",
    "#     good_videos, bad_videos = evaluate_videos(video_data_df)\n",
    "#     return good_videos, bad_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL RUN\n",
    "# run this if you want to scrape data and generate a dataset BE CAREFUL THIS USES MANY API CALLS\n",
    "# meta_df = main_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get initial search data (usually a list of channels)\n",
    "# out = request_channels(\"gaming\",\"5\",\"relevance\")\n",
    "# data = out['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get channel ids from search results\n",
    "# channel_ids = []\n",
    "# for channel in data:\n",
    "#     cur_channel_id = channel['snippet']['channelId']\n",
    "#     channel_ids.append(cur_channel_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get channel videos from a specific channel id\n",
    "# cur_channel_id = channel_ids[1] # arbitrary channel id for demo purposes\n",
    "# videos = request_channel_videos(cur_channel_id,\"5\",\"date\",None)\n",
    "# video_ids = []\n",
    "# for video in videos['items']:\n",
    "#     video_ids.append(video['id']['videoId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata = []\n",
    "# # use youtube videos api to get metadata about a single video, by video id\n",
    "# for cur_id in video_ids:\n",
    "#     cur_vid = request_video_details(cur_id)['items'][0]\n",
    "#     row = get_vid_stats(cur_vid)\n",
    "#     metadata.append(row)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a dataframe from the gathered metadata\n",
    "# columns = ['channelId','channelTitle','thumbnailLink','videoTitle','Date','Views','Likes','Dislikes','Comments']\n",
    "# df = pd.DataFrame(metadata,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # handpicked youtubers to analyze (channel ids)\n",
    "# pokeaimMD = 'UCbXuFrNGSKcmZY_5DwYz4Ew'\n",
    "# pewdiepie = 'UC-lHJZR3Gqxm24_Vd_AJ5Yw'\n",
    "# ninja = 'UCAW-NpUFkMyCNrvRSSGIvDQ'\n",
    "# abdallah = 'UCsDtTzkvGxxw95C4IOfZ7dw'\n",
    "# ytdan = 'UCXgNU9GtLPiE2dAYDwIQO6Q'\n",
    "# # possible idea is to just use most relevant to \"gaming\" search term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full pipeline analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_videos, bad_videos = analyze_channel(\"ninja\")\n",
    "# good_videos, bad_videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Example (Step-by-Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ninja_videos = request_channel_videos(ninja, \"30\", \"date\", None) # first 30 videos\n",
    "# next_page = ninja_videos['nextPageToken']\n",
    "# ninja_videos2 = request_channel_videos(ninja, \"30\", \"date\",next_page) # next 30 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ninja_videos = generate_video_data(ninja_videos)\n",
    "# ninja_thumbnails = generate_thumbnails(ninja_videos['thumbnailLink'].values)\n",
    "# good_videos, bad_videos = evaluate_videos(ninja_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check to see that output of full pipeline is similar to step by step process\n",
    "# good_videos, bad_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # analysis of next 30 ninja videos\n",
    "# more_ninja_videos = generate_video_data(ninja_videos2)\n",
    "# more_ninja_thumbnails = generate_thumbnails(more_ninja_videos['thumbnailLink'].values)\n",
    "# good_videos2, bad_videos2 = evaluate_videos(more_ninja_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually Displaying the good/bad thumbnails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # good thumbnails\n",
    "# for i in good_videos:\n",
    "#     display(ninja_thumbnails[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # more good videos\n",
    "# for i in good_videos2:\n",
    "#     display(more_ninja_thumbnails[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bad videos\n",
    "# for i in bad_videos:\n",
    "#     display(ninja_thumbnails[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # more bad videos\n",
    "# for i in bad_videos2:\n",
    "#     display(more_ninja_thumbnails[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
