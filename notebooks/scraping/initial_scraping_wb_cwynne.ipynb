{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Scrape Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We ideally want to scrape a dataset consisting of\n",
    "- video thumbnails\n",
    "- titles\n",
    "- views\n",
    "- parent channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YouTube API\n",
    "https://developers.google.com/youtube/v3/getting-started#quota\n",
    "\n",
    "Get credentials by going to https://console.developers.google.com/apis/dashboard?project=red-means-go\n",
    "- Look for \"YouTube Data API v3\" in the library tab and make sure it's enabled.\n",
    "- Select Credentials and get an api key\n",
    "\n",
    "Daily limit of 10,000 \"units\" worth of requests.\n",
    "- Different operations have different cost values, need to be careful what data we request.\n",
    "\n",
    "We can more efficiently get data by using the offered compressed gzip request format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install --upgrade google-auth-oauthlib google-auth-httplib2\n",
    "!pip install --upgrade google-api-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desired scraping code\n",
    "- config files to identify what categories of videos to scrape\n",
    "- what level of popularity to lower bound our videos to\n",
    "    - what measurement works for this? subscription to yearly average view count in relation to videos uploaded count?\n",
    "- possible inversion config option to instead opt for getting the least popular videos(?)\n",
    "- output to data/out/\n",
    "    - /thumbs -- a folder full of thumbnails with identifying labels (possibly gzip compressed?)\n",
    "    - videos.csv -- a .csv containing metadata on the videos that correspond to the thumbnails in the above folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible search parameters\n",
    "- Safesearch\n",
    "    - none\n",
    "    - moderate\n",
    "    - strict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import dateutil.relativedelta\n",
    "\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup and Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopes = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "with open('../../api_key.json') as json_file:\n",
    "    cred = json.load(json_file)\n",
    "api_key = cred['api_key']\n",
    "# Disable OAuthlib's HTTPS verification when running locally.\n",
    "# *DO NOT* leave this option enabled in production.\n",
    "os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run_search_result(q_term, num_search_results, videos_per_channel):\n",
    "    \n",
    "    # list of strings: \n",
    "    # returns the list of video/parent channel ids from a search result with the given query term\n",
    "    video_ids, parent_ids = iterate_search_results(q_term, num_search_results)\n",
    "    \n",
    "    # dictionary:\n",
    "    # returns a dictionary where the keys are the parent channel ids\n",
    "    # and the values are the video ids from the uploads playlist for that channel of length videos_per_channel\n",
    "    channel_videos_dic = populate_channel_game_videos(q_term, parent_ids, videos_per_channel)\n",
    "    print(\"------------------\")\n",
    "    print(\"unique channels gathered: \", len(channel_videos_dic.keys()))\n",
    "    print(\"------------------\")\n",
    "    \n",
    "    # list of dictionaries:\n",
    "    # Each dictionary corresponds to a video_id from the recent playlist in the game topic channel\n",
    "    # the number of dictionaries is equal to num_recent_videos.\n",
    "    #\n",
    "    # Each dictionary contains the video id, parent channel id, \n",
    "    # position in the YouTube game topic recent playlist, \n",
    "    # and list of videos from that channel on the speicified game.\n",
    "    print(\"------------------\")\n",
    "    print(\"Aggregating Results...\")\n",
    "    res = generate_result_dics(video_ids, parent_ids, channel_videos_dic)\n",
    "    print(\"Data Successfully scraped!\")\n",
    "    print(\"------------------\")\n",
    "    return res\n",
    "\n",
    "def full_run_topic_channel(q_term, num_recent_videos, videos_per_channel):\n",
    "    # string: \n",
    "    # Gets the id of the YouTube auto-generated topic channel for the q_term\n",
    "    topic_id = request_topic_id(q_term + \" topic\")\n",
    "    \n",
    "    # string: \n",
    "    # Gets the playlist id of the recent playlist in the game topic channel\n",
    "    topic_recent_playlist_id = request_recent_playlist_id(topic_id)\n",
    "    \n",
    "    # list of strings: \n",
    "    # returns the YouTube response object containing the specified amount of video_ids from the recent playlist\n",
    "    video_ids = get_video_ids(topic_recent_playlist_id, num_recent_videos)\n",
    "    \n",
    "    # list of strings:\n",
    "    # returns the parent channel of the videos in the game topic recent playlist\n",
    "    parent_ids = get_parent_channels(video_ids)\n",
    "    \n",
    "    # dictionary:\n",
    "    # returns a dictionary where the keys are the parent channel ids\n",
    "    # and the values are the video ids from the uploads playlist for that channel of length videos_per_channel\n",
    "    channel_videos_dic = populate_channel_game_videos(q_term, parent_ids, videos_per_channel)\n",
    "    \n",
    "    # list of dictionaries:\n",
    "    # Each dictionary corresponds to a video_id from the recent playlist in the game topic channel\n",
    "    # the number of dictionaries is equal to num_recent_videos.\n",
    "    #\n",
    "    # Each dictionary contains the video id, parent channel id, \n",
    "    # position in the YouTube game topic recent playlist, \n",
    "    # and list of videos from that channel on the speicified game.\n",
    "    res = generate_result_dics(video_ids, parent_ids, channel_videos_dic)\n",
    "    return res\n",
    "\n",
    "def generate_dataset(q_term, num_recent_videos, videos_per_channel):\n",
    "    start = datetime.now()\n",
    "    res = full_run_search_result(q_term, num_recent_videos, videos_per_channel)\n",
    "    end = datetime.now()\n",
    "    print(\"time elapsed:\", end-start)\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_result_dics(videos, parents, channel_videos):\n",
    "    all_results = []\n",
    "    for i in range(len(videos)):\n",
    "        out_dic = {\"video_id\": videos[i],\n",
    "                   \"position\": i,\n",
    "                   \"channel_id\": parents[i],\n",
    "                   \"channel_videos\": channel_videos[parents[i]]}\n",
    "        all_results.append(out_dic)\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def get_channel_game_videos(game, parent, num_vids):\n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,contentDetails\",\n",
    "        id=parent,\n",
    "        )\n",
    "    response = request.execute()\n",
    "    \n",
    "    game_vids = []\n",
    "    if num_vids < 50:\n",
    "        max_results = num_vids\n",
    "    else:\n",
    "        max_results = 50\n",
    "       \n",
    "    # initial first page result\n",
    "    uploads_id = response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    uploads_details = request_playlist_videos(uploads_id, max_results) \n",
    "    for vid_data in uploads_details['items']:\n",
    "        game_vids.append(vid_data['snippet']['resourceId']['videoId'])\n",
    "        if len(game_vids) == num_vids:\n",
    "            break  \n",
    "            \n",
    "    # if first page doesn't provide enough videos specified by num_vids, this iterates the pages\n",
    "    # until the length of game_vids matches num_vids\n",
    "    try:\n",
    "        next_token = uploads_details['nextPageToken']\n",
    "        while len(game_vids) < num_vids:\n",
    "            cur_page = request_playlist_videos(uploads_id, max_results, next_token)\n",
    "            for vid_data in cur_page['items']:\n",
    "                game_vids.append(vid_data['snippet']['resourceId']['videoId'])\n",
    "                if len(game_vids) == num_vids:\n",
    "                    break \n",
    "\n",
    "            next_token = cur_page['nextPageToken']\n",
    "            time.sleep(3) # trying to not overload the api\n",
    "    except:\n",
    "        print(\"Probably next page token error\")\n",
    "        print(uploads_details['items'][0]['snippet']['channelTitle'])\n",
    "    return game_vids\n",
    "\n",
    "\n",
    "def get_parent_channels(video_ids):\n",
    "    parent_channel_ids = []\n",
    "    for vid_id in video_ids:\n",
    "        vid_content = request_sparse_video_details(vid_id)\n",
    "        parent_channel = vid_content['items'][0]['snippet']['channelId']\n",
    "        parent_channel_ids.append(parent_channel)\n",
    "        \n",
    "    return parent_channel_ids\n",
    "\n",
    "\n",
    "def get_video_ids(playlist_id, num_vids):\n",
    "    recent_video_ids = []\n",
    "    max_results = 50\n",
    "    if num_vids < max_results:\n",
    "        playlist_details = request_playlist_videos(playlist_id, num_vids)\n",
    "    else:\n",
    "        playlist_details = request_playlist_videos(playlist_id, max_results)\n",
    "        \n",
    "    for vid_data in playlist_details['items']:\n",
    "        recent_video_ids.append(vid_data['snippet']['resourceId']['videoId'])\n",
    "        if len(recent_video_ids) == num_vids:\n",
    "            break         \n",
    "    \n",
    "    # extends recent video ids if the num_vids was larger than the initial page's results\n",
    "    print(playlist_details)\n",
    "    print(recent_video_ids)\n",
    "    next_token = playlist_details['nextPageToken']\n",
    "    while len(recent_video_ids) < num_vids:\n",
    "        cur_page = request_playlist_videos(playlist_id, max_results, next_token)\n",
    "        for vid_data in cur_page['items']:\n",
    "            recent_video_ids.append(vid_data['snippet']['resourceId']['videoId'])\n",
    "            if len(recent_video_ids) == num_vids:\n",
    "                break\n",
    "        next_token = cur_page['nextPageToken']\n",
    "        time.sleep(3)\n",
    "    return recent_video_ids\n",
    "\n",
    "def iterate_search_results(q_term, num_results):\n",
    "    print(\"------------------\")\n",
    "    print(\"Starting iteration of search results...\")\n",
    "    video_ids = []\n",
    "    parent_ids = []\n",
    "    max_results = 50\n",
    "    \n",
    "    if num_results < 50:\n",
    "        max_results = num_results\n",
    "        \n",
    "    init_search = search_result(q_term, max_results)\n",
    "    for vid_data in init_search['items']:\n",
    "        video_ids.append(vid_data['id']['videoId'])\n",
    "        parent_ids.append(vid_data['snippet']['channelId'])\n",
    "        if len(video_ids) == num_results:\n",
    "            break\n",
    "    print(\"Current results retrieved:\", len(video_ids), 100*len(video_ids)/num_results, \"%\")\n",
    "    try:\n",
    "        next_token = init_search['nextPageToken']\n",
    "        while len(video_ids) < num_results:\n",
    "            cur_page = search_result(q_term, max_results, next_token)\n",
    "            for vid_data in cur_page['items']:\n",
    "                video_ids.append(vid_data['id']['videoId'])\n",
    "                parent_ids.append(vid_data['snippet']['channelId'])\n",
    "                if len(video_ids) == num_results:\n",
    "                    break\n",
    "            next_token = cur_page['nextPageToken']\n",
    "            print(\"Current results retrieved:\", len(video_ids), 100*len(video_ids)/num_results, \"%\")\n",
    "    except:\n",
    "        print(\"No new pages. Returning current video ids and parent ids\")\n",
    "    print(\"Done iterating search results!\")\n",
    "    print(\"------------------\")\n",
    "    return video_ids, parent_ids\n",
    "\n",
    "def populate_channel_game_videos(game, parents, num_vids):\n",
    "    print(\"------------------\")\n",
    "    print(\"Starting retrieval of channel videos for\", len(parents), \"channels...\")\n",
    "    channel_videos = {}\n",
    "    counter = 0\n",
    "    for par_chan in parents:\n",
    "        if counter % 5 == 0:\n",
    "            print(\"Channels completed: \" + str(counter), 100*counter/len(parents), \"%\")\n",
    "        if par_chan not in channel_videos.keys():\n",
    "            channel_videos[par_chan] = get_channel_game_videos(game, par_chan, num_vids)\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter += 1\n",
    "    print(\"Done Retrieving Channel Videos!\")\n",
    "    print(\"------------------\")\n",
    "    return channel_videos\n",
    "\n",
    "def request_playlist_videos(playlist_id, num_results, page_token=None):\n",
    "    if page_token:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"snippet\",\n",
    "            maxResults=num_results,\n",
    "            playlistId=playlist_id,\n",
    "            pageToken=page_token\n",
    "        )\n",
    "    else:\n",
    "        request = youtube.playlistItems().list(\n",
    "                part=\"snippet\",\n",
    "                maxResults=num_results,\n",
    "                playlistId=playlist_id,\n",
    "            )\n",
    "    response = request.execute()\n",
    "    return response\n",
    "\n",
    "\n",
    "def request_recent_playlist_id(game_topic_channel):\n",
    "    request = youtube.channelSections().list(\n",
    "        part=\"snippet,contentDetails\",\n",
    "        channelId=game_topic_channel,\n",
    "        )\n",
    "    response = request.execute()\n",
    "    recent_playlist = None\n",
    "    for section in response['items']:\n",
    "        try: \n",
    "            if section['snippet']['localized']['title'] == \"Recent Videos\":\n",
    "                recent_playlist = section['contentDetails']['playlists'][0]\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "    return recent_playlist\n",
    "\n",
    "\n",
    "def request_sparse_video_details(video_id):\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey=api_key)\n",
    "    # note that this uses youtube.videos instead of youtube.search\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response \n",
    "\n",
    "\n",
    "def request_topic_id(q_term): \n",
    "    request = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            q=q_term,\n",
    "            maxResults=\"1\",\n",
    "            type=\"channel\",\n",
    "            order=\"relevance\"\n",
    "            )\n",
    "    response = request.execute()\n",
    "    game_topic_channel = response['items'][0]['snippet']['channelId']\n",
    "    return game_topic_channel\n",
    "\n",
    "def save_to_json(data, date, dir_path, fname, overwrite=False):  \n",
    "    out_dic = {\"date_scraped\": date,\n",
    "               \"data\": data\n",
    "              }\n",
    "    if os.path.exists(dir_path + fname) and not overwrite:\n",
    "        raise ValueError(\"Attempting to overwrite existing data. If you want to proceed pass overwrite=True\")\n",
    "    if os.path.isdir(dir_path):\n",
    "        with open(dir_path + fname, 'w') as outfile:\n",
    "            json.dump(out_dic, outfile)\n",
    "    else:\n",
    "        os.makedirs(dir_path)\n",
    "        with open(dir_path + fname, 'w') as outfile:\n",
    "            json.dump(out_dic, outfile) \n",
    "            \n",
    "def search_result(q_term, max_results, page_token=None):\n",
    "    d = datetime.now()\n",
    "    prev_month = d - dateutil.relativedelta.relativedelta(months=1)\n",
    "    prev_month_rfc = prev_month.isoformat('T') + \"Z\"\n",
    "    time.sleep(3)\n",
    "    if page_token:\n",
    "        request = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            q=q_term,\n",
    "            maxResults=max_results,\n",
    "            pageToken=page_token,\n",
    "            type=\"video\",\n",
    "            order=\"relevance\",\n",
    "            publishedAfter=prev_month_rfc\n",
    "            )\n",
    "    else:\n",
    "        request = youtube.search().list(\n",
    "                part=\"snippet\",\n",
    "                q=q_term,\n",
    "                maxResults=max_results,\n",
    "                type=\"video\",\n",
    "                order=\"relevance\",\n",
    "                publishedAfter=prev_month_rfc\n",
    "                )\n",
    "    response = request.execute()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_search_results = 200\n",
    "videos_per_channel = 100\n",
    "date = time.strftime(\"%m_%d_%y\",time.localtime())\n",
    "fname = \"scrape_\" + date + \".json\"\n",
    "dir_path = \"../../data/out/fortnite/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortnite = generate_dataset(\"fortnite\", number_of_search_results, videos_per_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the amount of videos for each channel\n",
    "prev_api = 683\n",
    "for channel in fortnite:\n",
    "    print(len(channel['channel_videos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dataset to json\n",
    "save_to_json(fortnite,date,dir_path,fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apex = generate_dataset(\"apex legends\", number_of_search_results, videos_per_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Functions / Code that is no longer in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def general_search(q_term, num_results, res_type, channel_id, order_type, page_token=None):\n",
    "#     youtube = googleapiclient.discovery.build(\n",
    "#         api_service_name, api_version, developerKey=api_key)\n",
    "    \n",
    "#     if page_token:\n",
    "#         request = youtube.search().list(\n",
    "#             part=\"snippet\",\n",
    "#             maxResults=num_results,\n",
    "#             type=res_type,\n",
    "#             channelId=channel_id,\n",
    "#             order=order_type,\n",
    "#             pageToken=page_token\n",
    "#         )\n",
    "#     else:\n",
    "#         request = youtube.search().list(\n",
    "#             part=\"snippet\",\n",
    "#             maxResults=num_results,\n",
    "#             type=res_type,\n",
    "#             channelId=channel_id,\n",
    "#             order=order_type\n",
    "#         )\n",
    "#     response = request.execute()\n",
    "#     return response\n",
    "\n",
    "# def request_channels(q_term, num_results,order_type):\n",
    "#     # all arguments must be strings, returns json object with list of channels\n",
    "#     youtube = googleapiclient.discovery.build(\n",
    "#         api_service_name, api_version, developerKey=api_key)\n",
    "    \n",
    "#     # Search parameters\n",
    "#     request = youtube.search().list(\n",
    "#         part=\"snippet\",\n",
    "#         q=q_term,\n",
    "#         maxResults=num_results,\n",
    "#         type=\"channel\",\n",
    "#         order=order_type\n",
    "#     )\n",
    "#     response = request.execute()\n",
    "\n",
    "#     return response\n",
    "\n",
    "# def request_channel_videos(channel_id, num_results, order_type, page_token):\n",
    "#     # all arguments must be strings, returns json object with list of videos from the given channel\n",
    "    \n",
    "#     youtube = googleapiclient.discovery.build(\n",
    "#         api_service_name, api_version, developerKey=api_key)\n",
    "#     if page_token:\n",
    "#         request = youtube.search().list(\n",
    "#             part=\"snippet\",\n",
    "#             maxResults=num_results,\n",
    "#             type=\"video\",\n",
    "#             channelId=channel_id,\n",
    "#             order=order_type,\n",
    "#             pageToken=page_token\n",
    "#         )\n",
    "#     else:\n",
    "#         request = youtube.search().list(\n",
    "#             part=\"snippet\",\n",
    "#             maxResults=num_results,\n",
    "#             type=\"video\",\n",
    "#             channelId=channel_id,\n",
    "#             order=order_type\n",
    "#         )\n",
    "#     response = request.execute()\n",
    "#     return response\n",
    "\n",
    "# def request_video_details(video_id):\n",
    "#     youtube = googleapiclient.discovery.build(\n",
    "#         api_service_name, api_version, developerKey=api_key)\n",
    "#     # note that this uses youtube.videos instead of youtube.search\n",
    "#     request = youtube.videos().list(\n",
    "#         part=\"snippet,contentDetails,statistics\",\n",
    "#         id=video_id\n",
    "#     )\n",
    "#     response = request.execute()\n",
    "#     return response\n",
    "\n",
    "# def get_vid_stats(vid):\n",
    "#     channel_id = vid['snippet']['channelId']\n",
    "#     channel_title = vid['snippet']['channelTitle']\n",
    "#     try:\n",
    "#         thumbnail_link = vid['snippet']['thumbnails']['maxres']['url']\n",
    "#     except:\n",
    "#         thumbnail_link = vid['snippet']['thumbnails']['high']['url']\n",
    "#     title = vid['snippet']['title']\n",
    "#     date = vid['snippet']['publishedAt']\n",
    "#     views = vid['statistics']['viewCount']\n",
    "#     likes = vid['statistics']['likeCount']\n",
    "#     dislikes = vid['statistics']['dislikeCount']\n",
    "#     comments = vid['statistics']['commentCount']\n",
    "#     stats = [channel_id, channel_title, thumbnail_link, title, date, views, likes, dislikes, comments]\n",
    "#     return stats\n",
    "\n",
    "# # full pipeline for scraping and generating dataframe\n",
    "# def main_pipeline():\n",
    "#     metadata = []\n",
    "#     # get initial search results (usually going to be a list of channels)\n",
    "#     out = request_channels(\"gaming\",\"5\",\"relevance\")\n",
    "#     data = out['items']\n",
    "#     # get channel ids from search results\n",
    "#     channel_ids = []\n",
    "#     for channel in data:\n",
    "#         cur_channel_id = channel['snippet']['channelId']\n",
    "#         channel_ids.append(cur_channel_id)\n",
    "#         # get channel videos from the current channel id (we can also choose to handpick channels / videos)\n",
    "#         videos = request_channel_videos(cur_channel_id,\"5\",\"date\",None)\n",
    "#         video_ids = []\n",
    "#         for video in videos['items']:\n",
    "#             cur_id = video['id']['videoId']\n",
    "#             video_ids.append(cur_id)\n",
    "#             # use youtube videos api to get metadata about a single video, by video id\n",
    "#             cur_vid = request_video_details(cur_id)['items'][0]\n",
    "#             row = get_vid_stats(cur_vid)\n",
    "#             metadata.append(row)\n",
    "#         time.sleep(1)\n",
    "\n",
    "#     # create a dataframe from the gathered metadata\n",
    "#     columns = ['channelId','channelTitle','thumbnailLink',\n",
    "#                'videoTitle','Date','Views',\n",
    "#                'Likes','Dislikes','Comments']\n",
    "#     df = pd.DataFrame(metadata,columns=columns)\n",
    "#     return df\n",
    "\n",
    "# def get_channel_id(q_term):\n",
    "#     # rough channel conversion function (may not work all the time)\n",
    "    \n",
    "#     init_search = request_channels(q_term,\"1\",\"relevance\")\n",
    "#     chan_id = init_search['items'][0]['snippet']['channelId']\n",
    "#     return chan_id\n",
    "\n",
    "# def generate_video_data(video_list):\n",
    "#     # generates video data for each video in given list and returns a formatted df of video statistics\n",
    "#     channel_data = []\n",
    "#     for vid in video_list['items']:\n",
    "#         cur_id = vid['id']['videoId']\n",
    "#         cur_vid = request_video_details(cur_id)['items'][0]\n",
    "#         row = get_vid_stats(cur_vid)\n",
    "#         channel_data.append(row)\n",
    "#     columns = ['channelId','channelTitle','thumbnailLink',\n",
    "#                'videoTitle','Date','Views',\n",
    "#                'Likes','Dislikes','Comments']\n",
    "#     df = pd.DataFrame(channel_data,columns=columns)\n",
    "#     df['Views'] = df['Views'].astype(int)\n",
    "#     df['Likes'] = df['Likes'].astype(int)\n",
    "#     df['Dislikes'] = df['Dislikes'].astype(int)\n",
    "#     df['Comments'] = df['Comments'].astype(int)\n",
    "#     return df\n",
    "\n",
    "# def generate_thumbnails(thumbnail_list):\n",
    "#     # just makes a request call to the thumbnail link\n",
    "#     imgs = []\n",
    "#     for link in thumbnail_list:\n",
    "#         response = requests.get(link)\n",
    "#         img = Image.open(BytesIO(response.content))\n",
    "#         imgs.append(img)\n",
    "#     return imgs\n",
    "\n",
    "# def evaluate_videos(df):\n",
    "#     # evaulated based on z score of views and likes currently, very rough approx\n",
    "#     avg_views = df['Views'].mean()\n",
    "#     avg_likes = df['Likes'].mean()\n",
    "#     avg_dislikes = df['Dislikes'].mean()\n",
    "#     avg_comments = df['Comments'].mean()\n",
    "    \n",
    "#     std_views = np.std(df['Views'])\n",
    "#     std_likes = np.std(df['Likes'])\n",
    "#     std_dislikes = np.std(df['Dislikes'])\n",
    "#     std_comments = np.std(df['Comments'])\n",
    "    \n",
    "#     z_views = df['Views'].apply(lambda x: (x - avg_views) / std_views)\n",
    "#     z_likes = df['Likes'].apply(lambda x: (x - avg_likes) / std_likes)\n",
    "#     z_dislikes = df['Dislikes'].apply(lambda x: (x - avg_dislikes) / std_dislikes)\n",
    "#     z_comments = df['Comments'].apply(lambda x: (x - avg_comments) / std_comments)\n",
    "    \n",
    "#     good_videos = []\n",
    "#     bad_videos = []\n",
    "#     for i in range(len(df)):\n",
    "#         if z_views[i] > .3 and z_likes[i] > .3:\n",
    "#             good_videos.append(i)\n",
    "#         if z_views[i] < -.5 and z_likes[i] < -.5:\n",
    "#             bad_videos.append(i)\n",
    "\n",
    "#     return good_videos, bad_videos\n",
    "\n",
    "# def analyze_channel(keyword):\n",
    "#     channel_id = get_channel_id(keyword)\n",
    "#     channel_videos = request_channel_videos(channel_id, \"30\", \"date\", None)\n",
    "#     video_data_df = generate_video_data(channel_videos)\n",
    "#     video_thumbnails = generate_thumbnails(video_data_df['thumbnailLink'].values)\n",
    "#     good_videos, bad_videos = evaluate_videos(video_data_df)\n",
    "#     return good_videos, bad_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL RUN\n",
    "# run this if you want to scrape data and generate a dataset BE CAREFUL THIS USES MANY API CALLS\n",
    "# meta_df = main_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get initial search data (usually a list of channels)\n",
    "# out = request_channels(\"gaming\",\"5\",\"relevance\")\n",
    "# data = out['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get channel ids from search results\n",
    "# channel_ids = []\n",
    "# for channel in data:\n",
    "#     cur_channel_id = channel['snippet']['channelId']\n",
    "#     channel_ids.append(cur_channel_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get channel videos from a specific channel id\n",
    "# cur_channel_id = channel_ids[1] # arbitrary channel id for demo purposes\n",
    "# videos = request_channel_videos(cur_channel_id,\"5\",\"date\",None)\n",
    "# video_ids = []\n",
    "# for video in videos['items']:\n",
    "#     video_ids.append(video['id']['videoId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata = []\n",
    "# # use youtube videos api to get metadata about a single video, by video id\n",
    "# for cur_id in video_ids:\n",
    "#     cur_vid = request_video_details(cur_id)['items'][0]\n",
    "#     row = get_vid_stats(cur_vid)\n",
    "#     metadata.append(row)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a dataframe from the gathered metadata\n",
    "# columns = ['channelId','channelTitle','thumbnailLink','videoTitle','Date','Views','Likes','Dislikes','Comments']\n",
    "# df = pd.DataFrame(metadata,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # handpicked youtubers to analyze (channel ids)\n",
    "# pokeaimMD = 'UCbXuFrNGSKcmZY_5DwYz4Ew'\n",
    "# pewdiepie = 'UC-lHJZR3Gqxm24_Vd_AJ5Yw'\n",
    "# ninja = 'UCAW-NpUFkMyCNrvRSSGIvDQ'\n",
    "# abdallah = 'UCsDtTzkvGxxw95C4IOfZ7dw'\n",
    "# ytdan = 'UCXgNU9GtLPiE2dAYDwIQO6Q'\n",
    "# # possible idea is to just use most relevant to \"gaming\" search term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full pipeline analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_videos, bad_videos = analyze_channel(\"ninja\")\n",
    "# good_videos, bad_videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Example (Step-by-Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ninja_videos = request_channel_videos(ninja, \"30\", \"date\", None) # first 30 videos\n",
    "# next_page = ninja_videos['nextPageToken']\n",
    "# ninja_videos2 = request_channel_videos(ninja, \"30\", \"date\",next_page) # next 30 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ninja_videos = generate_video_data(ninja_videos)\n",
    "# ninja_thumbnails = generate_thumbnails(ninja_videos['thumbnailLink'].values)\n",
    "# good_videos, bad_videos = evaluate_videos(ninja_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check to see that output of full pipeline is similar to step by step process\n",
    "# good_videos, bad_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # analysis of next 30 ninja videos\n",
    "# more_ninja_videos = generate_video_data(ninja_videos2)\n",
    "# more_ninja_thumbnails = generate_thumbnails(more_ninja_videos['thumbnailLink'].values)\n",
    "# good_videos2, bad_videos2 = evaluate_videos(more_ninja_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually Displaying the good/bad thumbnails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # good thumbnails\n",
    "# for i in good_videos:\n",
    "#     display(ninja_thumbnails[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # more good videos\n",
    "# for i in good_videos2:\n",
    "#     display(more_ninja_thumbnails[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bad videos\n",
    "# for i in bad_videos:\n",
    "#     display(ninja_thumbnails[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # more bad videos\n",
    "# for i in bad_videos2:\n",
    "#     display(more_ninja_thumbnails[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
